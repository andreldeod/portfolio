{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreldeod/portfolio/blob/main/code/ArcGIS_REST_Server/Download_vector_layer_from_Arc_REST_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading data from ArcGIS Rest Server\n",
        "## Instructions pulled from [agsout Github](https://github.com/tannerjt/AGStoShapefile) to download Chilean national archive of wetlands [layer](https://arcgis.mma.gob.cl/server/rest/services/SIMBIO/SIMBIO_HUMEDALES/MapServer/0)\n",
        "\n",
        "notebook by Andr√©"
      ],
      "metadata": {
        "id": "L3Gl7IGwkZKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a text file called services.txt\n",
        "You will use this to direct the code what to download, format of text file is:\n",
        "\n",
        " [URL of ArcGIS REST Services Directory] | [Layer ID] | [throttle time in milliseconds]\n",
        "\n",
        " The text file I used had the following:\n"
      ],
      "metadata": {
        "id": "UbiRtm8DAXnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://arcgis.mma.gob.cl/server/rest/services/SIMBIO/SIMBIO_HUMEDALES/MapServer/0|Inventario_Nacional_de_Humedales|5000"
      ],
      "metadata": {
        "id": "dbjyvf_So36V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Terminal set up and run\n",
        "\n",
        "Agsout is run on the terminal.\n",
        "\n",
        "1. If you don't already have it, install GDAL.\n",
        "\n",
        "2. Install nvm-windows (if not already installed):\n",
        "Download and install from nvm-windows [releases](https://github.com/coreybutler/nvm-windows/releases): nvm-setup.zip\n"
      ],
      "metadata": {
        "id": "i5SU6WDAtpKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code to run in terminal. Install Node.js and agsout\n",
        "# Install nvm-windows (if not already installed):\n",
        "\n",
        "#Install Node.js version 6.x:\n",
        "nvm install 6\n",
        "\n",
        "#Use Node.js version 6.x:\n",
        "nvm use 6\n",
        "\n",
        "\n",
        "#Verify the installation:\n",
        "node -v\n",
        "\n",
        "#Install agsout globally:\n",
        "npm install -g agsout\n"
      ],
      "metadata": {
        "id": "yHxpIYbbs2ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run agsout to download data and convert to shapefile or GeoJson\n",
        "\n",
        "```\n",
        "    agsout --help\n",
        "    agsout -s ./services.txt -o ./backupdir -S\n",
        "    #-s location of services text file\n",
        "    #-o directory to backup services\n",
        "    #-S optional shapefile output (requires gdal)\n",
        "```\n",
        "If -S not used then file will be downloaded as Geojson.\n"
      ],
      "metadata": {
        "id": "C-8C6wm7EHxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run your agsout command with the correct paths:\n",
        "agsout -s \"C:\\TerraCarbon\\chile_salt_marshes\\data\\services.txt\" -o \"C:\\TerraCarbon\\chile_salt_marshes\\data\\humedales_ArcGISREST\" -S"
      ],
      "metadata": {
        "id": "J-XZVSmjEGr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Following code is for post-processing, may not be needed\n",
        "If the agsout process does not fully work, you will end up with a bunch of json data. The following code was written to combine the jsons and convert into a single shapefile. However, agsout should automatically do this if the whole process is able to be completed."
      ],
      "metadata": {
        "id": "hDv1X16sAmo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run in WSL after opening Docker to connect notebook locally\n",
        "\n",
        "docker run --gpus=all -p 127.0.0.1:9000:8080 -v /mnt/c/TerraCarbon/chile_salt_marshes:/workspace us-docker.pkg.dev/colab-images/public/runtime"
      ],
      "metadata": {
        "id": "SEtjT74umOkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5erE6VQpbOq"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import shapefile\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Paths\n",
        "\n",
        "# Define the paths\n",
        "partials_dir = (\"/workspace/data/humedales_ArcGISREST/\"\n",
        "                \"Inventario_Nacional_de_Humedales/partials\")\n",
        "\n",
        "output_dir = (\"/workspace/data/humedales_ArcGISREST/\"\n",
        "              \"Inventario_Nacional_de_Humedales\")\n",
        "\n",
        "temp_shapefiles_dir = os.path.join(output_dir, \"temp_shapefiles\")\n",
        "merged_shapefile_path = os.path.join(output_dir, \"merged_humedales.shp\")\n",
        "\n",
        "# Create a temporary directory for shapefiles\n",
        "os.makedirs(temp_shapefiles_dir, exist_ok=True)\n",
        "\n",
        "humedales_dir=output_dir"
      ],
      "metadata": {
        "id": "hvqIWnJLpsCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check if malformed files\n",
        "\n",
        "\n",
        "# List to keep track of problematic files\n",
        "empty_files = []\n",
        "malformed_files = []\n",
        "\n",
        "# Read and validate all JSON files\n",
        "for filename in sorted(os.listdir(partials_dir)):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(partials_dir, filename)\n",
        "        with open(file_path, 'r') as f:\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                if not data:\n",
        "                    empty_files.append(filename)\n",
        "            except json.JSONDecodeError:\n",
        "                malformed_files.append(filename)\n",
        "\n",
        "print(f\"Empty files: {empty_files}\")\n",
        "print(f\"Malformed files: {malformed_files}\")\n",
        "\n",
        "# Optionally, remove empty or malformed files\n",
        "for filename in empty_files + malformed_files:\n",
        "    os.remove(os.path.join(partials_dir, filename))\n",
        "    print(f\"Removed file: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQkr920egvr5",
        "outputId": "c903a2be-df68-4004-86d9-aa376e743298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty files: []\n",
            "Malformed files: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert jsons to shapefiles then combine into one shapefile\n",
        "\n",
        "# List to track mismatched files\n",
        "mismatched_files = []\n",
        "\n",
        "# Function to process a single JSON file and create a shapefile\n",
        "def process_json_to_shapefile(json_file_path, shapefile_path):\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    # Create a shapefile writer object\n",
        "    w = shapefile.Writer(shapefile_path, shapeType=shapefile.POLYGON)\n",
        "\n",
        "    # Define the fields\n",
        "    w.field(\"OBJECTID\", \"N\")\n",
        "    w.field(\"COD_HUMEDA\", \"C\", size=50)\n",
        "    w.field(\"NOM_HUMDET\", \"C\", size=255)\n",
        "    w.field(\"NOM_HUMMASTER\", \"C\", size=255)\n",
        "    w.field(\"ORDEN_1\", \"N\")\n",
        "    w.field(\"ORDEN_2\", \"N\")\n",
        "    w.field(\"ORDEN_3\", \"N\")\n",
        "    w.field(\"ORDEN_4\", \"N\")\n",
        "    w.field(\"ORDEN_5\", \"N\")\n",
        "    w.field(\"HECTAREAS\", \"F\", decimal=10)\n",
        "    w.field(\"HLIMITEURBANO\", \"F\", decimal=10)\n",
        "    w.field(\"URL_SIMBIO\", \"C\", size=255)\n",
        "    w.field(\"COD_HUMMAS\", \"C\", size=50)\n",
        "\n",
        "    shapes = []\n",
        "    records = []\n",
        "    feature_count = 0\n",
        "\n",
        "    # Iterate over the features and collect shapes and records\n",
        "    for feature in data[\"features\"]:\n",
        "        geometry = feature[\"geometry\"]\n",
        "        properties = feature[\"properties\"]\n",
        "\n",
        "        # Add the polygon or multipolygon geometry\n",
        "        if geometry[\"type\"] == \"Polygon\":\n",
        "            shapes.append(geometry[\"coordinates\"])\n",
        "            records.append([\n",
        "                properties.get(\"OBJECTID\", None),\n",
        "                properties.get(\"COD_HUMEDA\", None),\n",
        "                properties.get(\"NOM_HUMDET\", None),\n",
        "                properties.get(\"NOM_HUMMASTER\", None),\n",
        "                properties.get(\"ORDEN_1\", None),\n",
        "                properties.get(\"ORDEN_2\", None),\n",
        "                properties.get(\"ORDEN_3\", None),\n",
        "                properties.get(\"ORDEN_4\", None),\n",
        "                properties.get(\"ORDEN_5\", None),\n",
        "                properties.get(\"HECTAREAS\", None),\n",
        "                properties.get(\"HLIMITEURBANO\", None),\n",
        "                properties.get(\"URL_SIMBIO\", None),\n",
        "                properties.get(\"COD_HUMMAS\", None)\n",
        "            ])\n",
        "            feature_count += 1\n",
        "        elif geometry[\"type\"] == \"MultiPolygon\":\n",
        "            # Flatten a list of multipolygons into a list of polygons\n",
        "            flattened_coords = [\n",
        "                polygon\n",
        "                for multipolygon in geometry[\"coordinates\"]\n",
        "                for polygon in multipolygon\n",
        "            ]\n",
        "\n",
        "            shapes.append(flattened_coords)\n",
        "            records.append([\n",
        "                properties.get(\"OBJECTID\", None),\n",
        "                properties.get(\"COD_HUMEDA\", None),\n",
        "                properties.get(\"NOM_HUMDET\", None),\n",
        "                properties.get(\"NOM_HUMMASTER\", None),\n",
        "                properties.get(\"ORDEN_1\", None),\n",
        "                properties.get(\"ORDEN_2\", None),\n",
        "                properties.get(\"ORDEN_3\", None),\n",
        "                properties.get(\"ORDEN_4\", None),\n",
        "                properties.get(\"ORDEN_5\", None),\n",
        "                properties.get(\"HECTAREAS\", None),\n",
        "                properties.get(\"HLIMITEURBANO\", None),\n",
        "                properties.get(\"URL_SIMBIO\", None),\n",
        "                properties.get(\"COD_HUMMAS\", None)\n",
        "            ])\n",
        "            feature_count += 1\n",
        "        else:\n",
        "            print(f\"Skipping non-polygon feature: {geometry['type']}\")\n",
        "            continue\n",
        "\n",
        "    # Check if the number of shapes and records match\n",
        "    if len(shapes) != feature_count:\n",
        "        print(f\"Warning: Number of shapes ({len(shapes)}) does not match number\"\n",
        "        \"of records ({feature_count}) in {os.path.basename(json_file_path)}\")\n",
        "        mismatched_files.append(json_file_path)\n",
        "        return\n",
        "\n",
        "    # Write the shapes and records to the shapefile\n",
        "    for shape_coords, record in zip(shapes, records):\n",
        "        w.poly(shape_coords)\n",
        "        w.record(*record)\n",
        "\n",
        "    # Save the shapefile\n",
        "    w.close()\n",
        "\n",
        "    # Create the projection file\n",
        "    prj_path = shapefile_path.replace(\".shp\", \".prj\")\n",
        "    with open(prj_path, \"w\") as prj:\n",
        "        epsg = (\n",
        "            'GEOGCS[\"WGS 84\",'\n",
        "            'DATUM[\"WGS_1984\",'\n",
        "            'SPHEROID[\"WGS 84\",6378137,298.257223563]],'\n",
        "            'PRIMEM[\"Greenwich\",0],'\n",
        "            'UNIT[\"degree\",0.0174532925199433]]'\n",
        "        )\n",
        "        prj.write(epsg)\n",
        "\n",
        "    print(f\"Shapefile saved to {os.path.basename(shapefile_path)}\")\n",
        "\n",
        "# Process each JSON file and create corresponding shapefiles\n",
        "json_files = sorted([f for f in os.listdir(partials_dir) if f.endswith('.json')])\n",
        "shapefile_paths = []\n",
        "for idx, json_file in enumerate(json_files):\n",
        "    json_file_path = os.path.join(partials_dir, json_file)\n",
        "    shapefile_path = os.path.join(temp_shapefiles_dir, f\"part_{idx}.shp\")\n",
        "    process_json_to_shapefile(json_file_path, shapefile_path)\n",
        "    shapefile_paths.append(shapefile_path)\n",
        "\n",
        "# Merge the shapefiles using GeoPandas\n",
        "gdfs = [gpd.read_file(shapefile_path) for shapefile_path in shapefile_paths]\n",
        "merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
        "\n",
        "# Save the merged shapefile\n",
        "merged_gdf.to_file(merged_shapefile_path, driver='ESRI Shapefile')\n",
        "\n",
        "print(f\"Merged shapefile saved to {os.path.basename(merged_shapefile_path)}\")\n",
        "\n",
        "# Delete the temporary shapefiles directory\n",
        "shutil.rmtree(temp_shapefiles_dir)\n",
        "print(f\"Deleted temporary shapefiles\"\n",
        "      \" directory {os.path.basename(temp_shapefiles_dir)}\")\n",
        "\n",
        "# Log mismatched files\n",
        "if mismatched_files:\n",
        "    print(f\"The following files had mismatched shapes\"\n",
        "          \" and records and were skipped:\")\n",
        "    for file in mismatched_files:\n",
        "        print(os.path.basename(file))\n"
      ],
      "metadata": {
        "id": "9JWoSUUh37K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save just one shapefile from one json file\n",
        "\n",
        "# Define the paths\n",
        "json_file_path = \"/workspace/data/humedales_ArcGISREST/Inventario_Nacional_de_Humedales/partials/935.json\"\n",
        "output_dir = \"/workspace/data/humedales_ArcGISREST/Inventario_Nacional_de_Humedales\"\n",
        "shapefile_path = os.path.join(output_dir, \"935.shp\")\n",
        "\n",
        "# Function to process a single JSON file and create a shapefile\n",
        "def process_json_to_shapefile(json_file_path, shapefile_path):\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    # Create a shapefile writer object\n",
        "    w = shapefile.Writer(shapefile_path, shapeType=shapefile.POLYGON)\n",
        "\n",
        "    # Define the fields\n",
        "    w.field(\"OBJECTID\", \"N\")\n",
        "    w.field(\"COD_HUMEDA\", \"C\", size=50)\n",
        "    w.field(\"NOM_HUMDET\", \"C\", size=255)\n",
        "    w.field(\"NOM_HUMMASTER\", \"C\", size=255)\n",
        "    w.field(\"ORDEN_1\", \"N\")\n",
        "    w.field(\"ORDEN_2\", \"N\")\n",
        "    w.field(\"ORDEN_3\", \"N\")\n",
        "    w.field(\"ORDEN_4\", \"N\")\n",
        "    w.field(\"ORDEN_5\", \"N\")\n",
        "    w.field(\"HECTAREAS\", \"F\", decimal=10)\n",
        "    w.field(\"HLIMITEURBANO\", \"F\", decimal=10)\n",
        "    w.field(\"URL_SIMBIO\", \"C\", size=255)\n",
        "    w.field(\"COD_HUMMAS\", \"C\", size=50)\n",
        "\n",
        "    shapes = []\n",
        "    records = []\n",
        "    feature_count = 0\n",
        "\n",
        "    # Iterate over the features and collect shapes and records\n",
        "    for feature in data[\"features\"]:\n",
        "        geometry = feature[\"geometry\"]\n",
        "        properties = feature[\"properties\"]\n",
        "\n",
        "        # Add the polygon or multipolygon geometry\n",
        "        if geometry[\"type\"] == \"Polygon\":\n",
        "            shapes.append(geometry[\"coordinates\"])\n",
        "            records.append([\n",
        "                properties.get(\"OBJECTID\", None),\n",
        "                properties.get(\"COD_HUMEDA\", None),\n",
        "                properties.get(\"NOM_HUMDET\", None),\n",
        "                properties.get(\"NOM_HUMMASTER\", None),\n",
        "                properties.get(\"ORDEN_1\", None),\n",
        "                properties.get(\"ORDEN_2\", None),\n",
        "                properties.get(\"ORDEN_3\", None),\n",
        "                properties.get(\"ORDEN_4\", None),\n",
        "                properties.get(\"ORDEN_5\", None),\n",
        "                properties.get(\"HECTAREAS\", None),\n",
        "                properties.get(\"HLIMITEURBANO\", None),\n",
        "                properties.get(\"URL_SIMBIO\", None),\n",
        "                properties.get(\"COD_HUMMAS\", None)\n",
        "            ])\n",
        "            feature_count += 1\n",
        "        elif geometry[\"type\"] == \"MultiPolygon\":\n",
        "            # Flatten a list of multipolygons into a list of polygons\n",
        "            flattened_coords = [\n",
        "                polygon\n",
        "                for multipolygon in geometry[\"coordinates\"]\n",
        "                for polygon in multipolygon\n",
        "            ]\n",
        "            shapes.append(flattened_coords)\n",
        "            records.append([\n",
        "                properties.get(\"OBJECTID\", None),\n",
        "                properties.get(\"COD_HUMEDA\", None),\n",
        "                properties.get(\"NOM_HUMDET\", None),\n",
        "                properties.get(\"NOM_HUMMASTER\", None),\n",
        "                properties.get(\"ORDEN_1\", None),\n",
        "                properties.get(\"ORDEN_2\", None),\n",
        "                properties.get(\"ORDEN_3\", None),\n",
        "                properties.get(\"ORDEN_4\", None),\n",
        "                properties.get(\"ORDEN_5\", None),\n",
        "                properties.get(\"HECTAREAS\", None),\n",
        "                properties.get(\"HLIMITEURBANO\", None),\n",
        "                properties.get(\"URL_SIMBIO\", None),\n",
        "                properties.get(\"COD_HUMMAS\", None)\n",
        "            ])\n",
        "            feature_count += 1\n",
        "        else:\n",
        "            print(f\"Skipping non-polygon feature: {geometry['type']}\")\n",
        "            continue\n",
        "\n",
        "    # Check if the number of shapes and records match\n",
        "    if len(shapes) != feature_count:\n",
        "        print(f\"Warning: Number of shapes ({len(shapes)}) does not match number\"\n",
        "        \"of records ({feature_count}) in {os.path.basename(json_file_path)}\")\n",
        "        return\n",
        "\n",
        "    # Write the shapes and records to the shapefile\n",
        "    for shape_coords, record in zip(shapes, records):\n",
        "        w.poly(shape_coords)\n",
        "        w.record(*record)\n",
        "\n",
        "    # Save the shapefile\n",
        "    w.close()\n",
        "\n",
        "    # Create the projection file\n",
        "    prj_path = shapefile_path.replace(\".shp\", \".prj\")\n",
        "    with open(prj_path, \"w\") as prj:\n",
        "        epsg = (\n",
        "            'GEOGCS[\"WGS 84\",'\n",
        "            'DATUM[\"WGS_1984\",'\n",
        "            'SPHEROID[\"WGS 84\",6378137,298.257223563]],'\n",
        "            'PRIMEM[\"Greenwich\",0],'\n",
        "            'UNIT[\"degree\",0.0174532925199433]]'\n",
        "        )\n",
        "        prj.write(epsg)\n",
        "\n",
        "    print(f\"Shapefile saved to {os.path.basename(shapefile_path)}\")\n",
        "\n",
        "# Process the specific JSON file\n",
        "process_json_to_shapefile(json_file_path, shapefile_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14nvxVUJCE6L",
        "outputId": "97d142b0-0cc8-415c-e3ef-764cc3453679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapefile saved to 935.shp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "#Additional potentially helpful code\n"
      ],
      "metadata": {
        "id": "fUOnHKa6kSQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract coded values for ORDEN fields from ArcGIS REST service\n",
        "#Script to extract coded values for ORDEN fields from ArcGIS REST service\n",
        "#import requests\n",
        "\n",
        "# URL of the ArcGIS REST service layer\n",
        "url = ('https://arcgis.mma.gob.cl/server/rest/services/SIMBIO/'\n",
        "       'SIMBIO_HUMEDALES/MapServer/0')\n",
        "\n",
        "# Function to get layer information\n",
        "def get_layer_info(url):\n",
        "    try:\n",
        "        response = requests.get(f'{url}?f=json')\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching layer info: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to print coded values for a field\n",
        "def print_coded_values(field_name, layer_info):\n",
        "    if not layer_info:\n",
        "        return\n",
        "    for field in layer_info['fields']:\n",
        "        if field['name'] == field_name:\n",
        "            print(f\"Coded values for {field_name}:\")\n",
        "            if 'domain' in field and 'codedValues' in field['domain']:\n",
        "                for coded_value in field['domain']['codedValues']:\n",
        "                    code = coded_value['code']\n",
        "                    description = coded_value['name']\n",
        "                    print(f\"  {code}: {description}\")\n",
        "            else:\n",
        "                print(\"  No coded values found for this field.\")\n",
        "\n",
        "# Fetch layer info\n",
        "layer_info = get_layer_info(url)\n",
        "\n",
        "# Print coded values for each ORDEN field\n",
        "print_coded_values('ORDEN_1', layer_info)\n",
        "print_coded_values('ORDEN_2', layer_info)\n",
        "print_coded_values('ORDEN_3', layer_info)\n",
        "print_coded_values('ORDEN_4', layer_info)\n",
        "print_coded_values('ORDEN_5', layer_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1YWhenGo3tu",
        "outputId": "348e75f6-9412-4105-edde-fde8e91e3648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coded values for ORDEN_1:\n",
            "  10: ARTIFICIALES\n",
            "  20: CONTINENTALES\n",
            "  30: MARINOS Y COSTEROS\n",
            "  999: SIN CLASIFICAR\n",
            "Coded values for ORDEN_2:\n",
            "  10: ALMACENAMIENTO\n",
            "  20: LACUSTRES\n",
            "  30: PALUSTRES\n",
            "  40: RIBERENOS\n",
            "  50: ESTUARINOS\n",
            "  60: MARINOS\n",
            "  70: LAGO SALADO\n",
            "  80: EXPLOTACION DE SAL\n",
            "  90: LAGUNAR\n",
            "  999: SIN CLASIFICAR\n",
            "Coded values for ORDEN_3:\n",
            "  10: ALMACENAMIENTO\n",
            "  20: BOSCOSOS\n",
            "  30: EMERGENTES\n",
            "  40: EXPLOTACION DE SAL\n",
            "  50: INTERMAREALES\n",
            "  60: LAGO SALADO\n",
            "  70: LAGUNAR\n",
            "  80: PERMANENTES\n",
            "  90: SUBMAREALES\n",
            "  100: TEMPORALES\n",
            "  999: SIN CLASIFICAR\n",
            "Coded values for ORDEN_4:\n",
            "  10: ANDINOS\n",
            "  20: DELTAS INTERIORES\n",
            "  30: EMBALSE\n",
            "  40: ESTACIONALES\n",
            "  50: ESTERO\n",
            "  60: ESTUARIOS\n",
            "  70: INTERMAREALES\n",
            "  80: IRREGULARES\n",
            "  90: LAGO\n",
            "  100: LAGO SALADO\n",
            "  110: LAGUNA\n",
            "  120: LAGUNA SALADA\n",
            "  130: MALLINES\n",
            "  140: PERMANENTES\n",
            "  150: PLAYAS\n",
            "  160: RIO\n",
            "  170: SALARES\n",
            "  180: SALINAS\n",
            "  190: TRANQUE\n",
            "  200: TURBERAS\n",
            "  999: SIN CLASIFICAR\n",
            "Coded values for ORDEN_5:\n",
            "  10: BOFEDAL\n",
            "  20: DELTAS INTERIORES\n",
            "  30: EMBALSE\n",
            "  40: ESTACIONALES\n",
            "  50: ESTERO\n",
            "  60: ESTUARIOS\n",
            "  70: GEISER\n",
            "  80: INTERMAREALES\n",
            "  90: IRREGULARES\n",
            "  100: LAGO\n",
            "  110: LAGO SALADO\n",
            "  120: LAGUNA\n",
            "  130: LAGUNA SALADA\n",
            "  140: MALLINES\n",
            "  150: PERDIDA\n",
            "  160: PERMANENTES\n",
            "  170: PLAYAS\n",
            "  180: RIO\n",
            "  190: SALARES\n",
            "  200: SALINAS\n",
            "  210: TRANQUE\n",
            "  220: TURBERAS\n",
            "  230: VEGA\n",
            "  999: SIN CLASIFICAR\n",
            "  240: TURBERA DE SPHAGNUM\n",
            "  250: TURBERA NATURAL\n",
            "  260: TURBERA EN ALTURA\n",
            "  270: TURBERA ANTROPOGENICA\n",
            "  280: TURBERA EN INTERFASE ESTUARINA\n",
            "  290: TURBERA EXPLOTADA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Replace ORDEN attribute code with names\n",
        "# This script replaces number attributes in ORDEN_ fields with their coded\n",
        "# values (with only the first letter capitalized and including translations)\n",
        "# in a shapefile.\n",
        "\n",
        "#import geopandas as gpd\n",
        "\n",
        "humedales_dir = (\"/workspace/data/humedales_ArcGISREST/\"\n",
        "              \"Inventario_Nacional_de_Humedales\")\n",
        "\n",
        "# Path to the input shapefile\n",
        "input_shapefile = (f'{humedales_dir}/Inventario_Nacional_de_Humedales_2024'\n",
        "                   '/Inventario_Nacional_de_Humedales.shp')\n",
        "\n",
        "# Path to the output shapefile\n",
        "output_shapefile =(f'{humedales_dir}/Inventario_Nacional_de_Humedales_withNames'\n",
        "                    '/Inventario_Nacional_de_Humedales.shp')\n",
        "\n",
        "# Dictionary with coded values and their translations for each ORDEN field\n",
        "orden_codes = {\n",
        "    'ORDEN_1': {\n",
        "        10: 'Artificiales - Artificial', 20: 'Continentales - Continental',\n",
        "        30: 'Marinos y costeros - Marine and coastal',\n",
        "        999: 'Sin clasificar - Unclassified'\n",
        "    },\n",
        "    'ORDEN_2': {\n",
        "        10: 'Almacenamiento - Storage', 20: 'Lacustres - Lacustrine',\n",
        "        30: 'Palustres - Palustrine', 40: 'Riberenos - Riparian',\n",
        "        50: 'Estuarinos - Estuarine', 60: 'Marinos - Marine',\n",
        "        70: 'Lago salado - Salt lake',\n",
        "        80: 'Explotacion de sal - Salt exploitation',\n",
        "        90: 'Lagunar - Lagoonal', 999: 'Sin clasificar - Unclassified'\n",
        "    },\n",
        "    'ORDEN_3': {\n",
        "        10: 'Almacenamiento - Storage', 20: 'Boscosos - Forested',\n",
        "        30: 'Emergentes - Emergent',\n",
        "        40: 'Explotacion de sal - Salt exploitation',\n",
        "        50: 'Intermareales - Intertidal', 60: 'Lago salado - Salt lake',\n",
        "        70: 'Lagunar - Lagoonal', 80: 'Permanentes - Permanent',\n",
        "        90: 'Submareales - Subtidal', 100: 'Temporales - Temporary',\n",
        "        999: 'Sin clasificar - Unclassified'\n",
        "    },\n",
        "    'ORDEN_4': {\n",
        "        10: 'Andinos - Andean', 20: 'Deltas interiores - Interior deltas',\n",
        "        30: 'Embalse - Reservoir', 40: 'Estacionales - Seasonal',\n",
        "        50: 'Estero - Estuary', 60: 'Estuarios - Estuaries',\n",
        "        70: 'Intermareales - Intertidal', 80: 'Irregulares - Irregular',\n",
        "        90: 'Lago - Lake', 100: 'Lago salado - Salt lake',\n",
        "        110: 'Laguna - Lagoon', 120: 'Laguna salada - Salt lagoon',\n",
        "        130: 'Mallines - Mallines', 140: 'Permanentes - Permanent',\n",
        "        150: 'Playas - Beaches', 160: 'Rio - River',\n",
        "        170: 'Salares - Salt flats',\n",
        "        180: 'Salinas - Salt mines', 190: 'Tranque - Dam',\n",
        "        200: 'Turberas - Peatlands',\n",
        "        999: 'Sin clasificar - Unclassified'\n",
        "    },\n",
        "    'ORDEN_5': {\n",
        "        10: 'Bofedal - Bofedal', 20: 'Deltas interiores - Interior deltas',\n",
        "        30: 'Embalse - Reservoir', 40: 'Estacionales - Seasonal',\n",
        "        50: 'Estero - Estuary', 60: 'Estuarios - Estuaries',\n",
        "        70: 'Geiser - Geyser', 80: 'Intermareales - Intertidal',\n",
        "        90: 'Irregulares - Irregular', 100: 'Lago - Lake',\n",
        "        110: 'Lago salado - Salt lake', 120: 'Laguna - Lagoon',\n",
        "        130: 'Laguna salada - Salt lagoon', 140: 'Mallines - Mallines',\n",
        "        150: 'Perdida - Lost', 160: 'Permanentes - Permanent',\n",
        "        170: 'Playas - Beaches', 180: 'Rio - River',\n",
        "        190: 'Salares - Salt flats',\n",
        "        200: 'Salinas - Salt mines', 210: 'Tranque - Dam',\n",
        "        220: 'Turberas - Peatlands', 230: 'Vega - Meadow',\n",
        "        240: 'Turbera de sphagnum - Sphagnum peat bog',\n",
        "        250: 'Turbera natural - Natural peat bog',\n",
        "        260: 'Turbera en altura - Highland peat bog',\n",
        "        270: 'Turbera antropogenica - Anthropogenic peat bog',\n",
        "        280: 'Turbera en interfase estuarina - Estuarine interface peat bog',\n",
        "        290: 'Turbera explotada - Exploited peat bog',\n",
        "        999: 'Sin clasificar - Unclassified'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to replace the numeric codes with their corresponding values\n",
        "def replace_orden_codes(df, field, codes):\n",
        "    df[field] = df[field].map(codes).fillna(df[field])\n",
        "    return df\n",
        "\n",
        "# Load the shapefile\n",
        "gdf = gpd.read_file(input_shapefile)\n",
        "\n",
        "# Replace the codes for each ORDEN field\n",
        "for field in orden_codes.keys():\n",
        "    if field in gdf.columns:\n",
        "        gdf = replace_orden_codes(gdf, field, orden_codes[field])\n",
        "\n",
        "# Save the modified shapefile\n",
        "gdf.to_file(output_shapefile)\n",
        "\n",
        "print(\"Shapefile processing completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KY8oBr84oVn",
        "outputId": "5f52466e-2b77-40e9-b387-2644f201f560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapefile processing completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get bounding box of geojson\n",
        "\n",
        "# Function to get the bounding box from the AOI GeoJSON file\n",
        "def get_aoi_extent(aoi_path):\n",
        "    gdf = gpd.read_file(aoi_path)\n",
        "    bounds = gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
        "    return bounds\n",
        "\n",
        "# URL for the ArcGIS REST service query\n",
        "url = \"https://arcgis.mma.gob.cl/server/rest/services/SIMBIO/SIMBIO_HUMEDALES/MapServer/0/query\"\n",
        "\n",
        "# Path to the AOI GeoJSON file\n",
        "aoi_path = \"/workspace/data/AOI/AOI.geojson\"\n",
        "\n",
        "# Get the bounding box from the AOI\n",
        "minx, miny, maxx, maxy = get_aoi_extent(aoi_path)\n",
        "\n",
        "# Parameters for the initial query to get the object IDs\n",
        "initial_params = {\n",
        "    \"geometry\": f\"{minx},{miny},{maxx},{maxy}\",\n",
        "    \"geometryType\": \"esriGeometryEnvelope\",\n",
        "    \"spatialRel\": \"esriSpatialRelIntersects\",\n",
        "    \"where\": \"1=1\",\n",
        "    \"returnIdsOnly\": \"true\",\n",
        "    \"f\": \"json\"\n",
        "}\n",
        "\n",
        "# Send the initial request to get object IDs\n",
        "initial_response = requests.get(url, params=initial_params)\n",
        "\n",
        "# Debug: Print the initial response content\n",
        "print(\"Initial Response Status Code:\", initial_response.status_code)\n",
        "print(\"Initial Response Text:\", initial_response.text)\n",
        "\n",
        "if initial_response.status_code == 200:\n",
        "    object_ids = initial_response.json().get('objectIds', None)\n",
        "\n",
        "    if object_ids is None:\n",
        "        print(\"No objectIds returned in the response.\")\n",
        "    else:\n",
        "        max_record_count = 1000  # Set the maxRecordCount parameter\n",
        "\n",
        "        all_features = []\n",
        "\n",
        "        def fetch_data(subset_ids, retries=3):\n",
        "            params = {\n",
        "                \"objectIds\": \",\".join(map(str, subset_ids)),\n",
        "                \"outFields\": \"*\",\n",
        "                \"f\": \"geojson\",\n",
        "                \"returnGeometry\": \"true\"\n",
        "            }\n",
        "\n",
        "            for attempt in range(retries):\n",
        "                response = requests.get(url, params=params)\n",
        "                if response.status_code == 200:\n",
        "                    return response.json().get('features', [])\n",
        "                else:\n",
        "                    print(f\"Failed attempt {attempt + 1} to download data chunk. HTTP status code: {response.status_code}\")\n",
        "                    time.sleep(2)  # Wait before retrying\n",
        "\n",
        "            return []\n",
        "\n",
        "        # Paginate through the records\n",
        "        for i in range(0, len(object_ids), max_record_count):\n",
        "            subset_ids = object_ids[i:i+max_record_count]\n",
        "            features = fetch_data(subset_ids)\n",
        "            all_features.extend(features)\n",
        "\n",
        "        if all_features:\n",
        "            geojson_data = {\n",
        "                \"type\": \"FeatureCollection\",\n",
        "                \"features\": all_features\n",
        "            }\n",
        "            # Save the combined features as a GeoJSON file in /workspace directory\n",
        "            output_path = \"/workspace/Inventario_Nacional_de_Humedales_clipped.geojson\"\n",
        "            with open(output_path, \"w\") as file:\n",
        "                json.dump(geojson_data, file)\n",
        "            print(f\"Data downloaded successfully and saved to {output_path}\")\n",
        "        else:\n",
        "            print(\"No data was downloaded.\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve object IDs. \" \\\n",
        "          f\"HTTP status code: {initial_response.status_code}\")\n",
        "    print(f\"Response: {initial_response.text}\")\n"
      ],
      "metadata": {
        "id": "wuindSgXplTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72597ec0-98b2-4bca-8f10-d0b85948c02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Response Status Code: 200\n",
            "Initial Response Text: {\"objectIdFieldName\":\"OBJECTID\",\"objectIds\":null}\n",
            "No objectIds returned in the response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print bounding box of geojson\n",
        "\n",
        "# Path to the AOI GeoJSON file\n",
        "aoi_path = \"/workspace/data/AOI/AOI.geojson\"\n",
        "\n",
        "# Load AOI GeoJSON file and get bounding box\n",
        "gdf = gpd.read_file(aoi_path)\n",
        "bounds = gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
        "\n",
        "print(\"AOI Bounding Box:\", bounds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ol0MxRnHYE",
        "outputId": "92dd3d19-4210-4fa7-aefa-7c9d070ab2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AOI Bounding Box: [-8330898.5987 -5475824.89   -7884401.6909 -4361699.6819]\n"
          ]
        }
      ]
    }
  ]
}